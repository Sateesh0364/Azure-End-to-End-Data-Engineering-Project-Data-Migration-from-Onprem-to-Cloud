{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to change a colmn name like FirstName to First_name\n",
    "We are placing underscore symbol between two coloumnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for understanding Iam taking one table and doing\n",
    "after this i will use for loop to do for all tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.ls('mnt/silver/SalesLT/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.ls('mnt/gold/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '/mnt/silver/SalesLT/Address/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read. format('delta').load(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql. functions import col, regexp_replace\n",
    "\n",
    "# Get the list of column names\n",
    "column_names = df.columns\n",
    "\n",
    "for old_col_name in column_names:\n",
    "    # Convert column name from ColumnName to Column_Name format\n",
    "    new_col_name = \"\".join([\"_\" + char if char. isupper() and not old_col_name[i - 1].isupper() else char for i, char in enumerate(old_col_name)]).lstrip(\"_\")\n",
    "\n",
    "    # Change the column name using withColumnRenamed and regexp_replace\n",
    "    df = df.withColumnRenamed(old_col_name, new_col_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing transformation for all tables (Changing Col Names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = []\n",
    "\n",
    "for i in dbutils.fs.ls('mnt/silver/SalesLT/'):\n",
    "    table_name.append(i.name.split('/')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in table_name:\n",
    "path = '/mnt/silver/SalesLT/' + name\n",
    "print (path)\n",
    "df = spark.read.format('delta').load(path)\n",
    "\n",
    "# Get the list of column names\n",
    "column_names = df.columns\n",
    "\n",
    "for old_col_name in column_names:\n",
    "    # Convert column name from ColumnName to Column_Name format\n",
    "    new_col_name = \"\".join([\"_\" + char if char. isupper() and not old_col_name[i - 1].isupper() else char for i, char in enumerate(old_col_name)]).lstrip(\"_\")\n",
    "\n",
    "    # Change the column name using withColumnRenamed and regexp_replace\n",
    "    df =df.withColumnRenamed(old_col_name, new_col_name)\n",
    "\n",
    "output_path ='/mnt/gold/SalesLT/' +name+'/'\n",
    "df.write.format('delta').mode(\"overwrite\").save(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have data in golder with two level of transformations\n",
    "one with date tramnsformations and other Col names tarnsformations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
